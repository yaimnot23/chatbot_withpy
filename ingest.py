import pandas as pd
import os
import shutil
import time
from tqdm import tqdm
from dotenv import load_dotenv
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain_chroma import Chroma
from langchain_core.documents import Document

# 1. í™˜ê²½ì„¤ì • ë¡œë“œ (.env íŒŒì¼ì˜ GOOGLE_API_KEY ë¡œë“œ)
load_dotenv()

def ingest_data():
    # 2. ê¸°ì¡´ DB ì‚­ì œ (ê¹¨ë—í•˜ê²Œ ìƒˆë¡œ ë§Œë“¤ê¸° ìœ„í•´)
    db_path = "./db"
    if os.path.exists(db_path):
        shutil.rmtree(db_path)
        print(f"ğŸ§¹ ê¸°ì¡´ DB í´ë”('{db_path}')ë¥¼ ì‚­ì œí–ˆìŠµë‹ˆë‹¤.")

    # 3. ì—‘ì…€ íŒŒì¼ ì„¤ì •
    excel_file_path = "data/univer_data.xlsx"
    if not os.path.exists(excel_file_path):
        print(f"âŒ '{excel_file_path}' íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
        return

    documents = []
    print(f"ğŸ“‚ '{excel_file_path}' íŒŒì¼ì„ ì½ëŠ” ì¤‘...")

    try:
        # ëª¨ë“  ì‹œíŠ¸ ì½ê¸°
        xls = pd.ExcelFile(excel_file_path)
        
        for sheet_name in xls.sheet_names:
            print(f"ğŸ“„ '{sheet_name}' ì‹œíŠ¸ ì²˜ë¦¬ ì¤‘...")
            # header=4ëŠ” ì‚¬ìš©ì ì´ì „ ì½”ë“œ ê¸°ì¤€ (ë°ì´í„° ì‹œì‘ ìœ„ì¹˜ì— ë”°ë¼ ì¡°ì • ê°€ëŠ¥)
            df = pd.read_excel(xls, sheet_name=sheet_name, header=4)
            
            # NaN ë°ì´í„°ë¥¼ ë¹ˆ ë¬¸ìì—´ë¡œ ì²˜ë¦¬
            df = df.fillna("")

            for _, row in df.iterrows():
                # 'ëŒ€í•™êµ'ì™€ 'ì „ê³µ' ì»¬ëŸ¼ ì°¾ê¸° (ìœ ì—°í•˜ê²Œ ëŒ€ì‘)
                univ = str(row.get('ëŒ€í•™êµ', row.get('ëŒ€í•™', ''))).strip()
                major = str(row.get('ì „ê³µ', row.get('ëª¨ì§‘ë‹¨ìœ„(ì „ê³µ)', row.get('ëª¨ì§‘ë‹¨ìœ„', '')))).strip()
                
                if not univ or not major:
                    continue

                category = str(row.get('ê³„ì—´', ''))
                region = f"{row.get('ì‹œë„','')} {row.get('ì‹œêµ°','')}".strip()
                target_score = str(row.get('ì ì •ì ìˆ˜', 'ì •ë³´ì—†ìŒ'))
                est_score = str(row.get('ì˜ˆìƒì ìˆ˜', 'ì •ë³´ì—†ìŒ'))

                # ê²€ìƒ‰ ì‹œ ì‚¬ìš©ë  í…ìŠ¤íŠ¸ êµ¬ì„±
                content = (
                    f"[{sheet_name}] {univ} {major} ({category}) ì…ì‹œ ì •ë³´. "
                    f"ì§€ì—­: {region}, ëª¨ì§‘êµ°: {row.get('ëª¨ì§‘êµ°','')}, ì •ì›: {row.get('ì •ì›','')}ëª…. "
                    f"ì ì • ì ìˆ˜: {target_score}ì , ì˜ˆìƒ ì ìˆ˜: {est_score}ì . "
                    f"ë°˜ì˜ë¹„ìœ¨: êµ­ì–´ {row.get('êµ­ì–´êµ¬ì„±ë¹„','')}, ìˆ˜í•™ {row.get('ìˆ˜í•™êµ¬ì„±ë¹„','')}, "
                    f"ì˜ì–´ {row.get('ì˜ì–´êµ¬ì„±ë¹„','')}, íƒêµ¬ {row.get('íƒêµ¬êµ¬ì„±ë¹„','')}."
                )
                
                # ë©”íƒ€ë°ì´í„° ì €ì¥ (ë¶„ì„ ì—”ì§„ì—ì„œ í™œìš©)
                metadata = {
                    "source": f"{univ} {major}",
                    "univ": univ,
                    "major": major,
                    "sheet": sheet_name,
                    "ëˆ„ë°±": str(row.get('ëˆ„ë°±', '')).strip(),
                    "ì ì •ì ìˆ˜": str(row.get('ì ì •ì ìˆ˜', '')).strip(),
                    "êµ­ì–´ë¹„ì¤‘": str(row.get('êµ­ì–´êµ¬ì„±ë¹„', '')).strip(),
                    "ìˆ˜í•™ë¹„ì¤‘": str(row.get('ìˆ˜í•™êµ¬ì„±ë¹„', '')).strip(),
                    "íƒêµ¬ë¹„ì¤‘": str(row.get('íƒêµ¬êµ¬ì„±ë¹„', '')).strip()
                }
                documents.append(Document(page_content=content, metadata=metadata))

        print(f"âœ… ì´ {len(documents)}ê°œì˜ ë¬¸ì„œë¥¼ ìƒì„±í–ˆìŠµë‹ˆë‹¤.")

        # 4. ë²¡í„° DB ì €ì¥ (Batch Processing)
        print("ğŸ’¾ ë²¡í„° DB(Chroma)ì— ì €ì¥ ì¤‘... (Rate Limit ë°©ì§€ë¥¼ ìœ„í•´ ì²œì²œíˆ ì§„í–‰í•©ë‹ˆë‹¤)")
        
        embeddings = GoogleGenerativeAIEmbeddings(model="text-embedding-004")
        
        vectorstore = Chroma(
            persist_directory=db_path,
            embedding_function=embeddings
        )

        batch_size = 100  # ìœ ë£Œ ë²„ì „ì´ë¯€ë¡œ ë°°ì¹˜ ì‚¬ì´ì¦ˆ í™•ëŒ€
        for i in tqdm(range(0, len(documents), batch_size), desc="ì €ì¥ ì§„í–‰ë¥ "):
            batch = documents[i : i + batch_size]
            
            # ìœ ë£Œ ë²„ì „ì€ ì†ë„ ì œí•œì´ ê±°ì˜ ì—†ìœ¼ë¯€ë¡œ ì¦‰ì‹œ ì²˜ë¦¬
            max_retries = 3
            for attempt in range(max_retries):
                try:
                    vectorstore.add_documents(batch)
                    break 
                except Exception as e:
                    if "429" in str(e) or "RESOURCE_EXHAUSTED" in str(e):
                        wait_time = (attempt + 1) * 2 # ëŒ€ê¸° ì‹œê°„ ëŒ€í­ ë‹¨ì¶•
                        print(f"\nâš ï¸ Rate Limit ë„ë‹¬! {wait_time}ì´ˆ í›„ ì¬ì‹œë„í•©ë‹ˆë‹¤... ({attempt+1}/{max_retries})")
                        time.sleep(wait_time)
                    else:
                        raise e
            
            # ë³„ë„ì˜ íœ´ì‹ ì‹œê°„ ì œê±°

        print(f"ğŸ‰ ëª¨ë“  ë°ì´í„°ê°€ '{db_path}' í´ë”ì— ì„±ê³µì ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")

    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")

if __name__ == "__main__":
    ingest_data()
